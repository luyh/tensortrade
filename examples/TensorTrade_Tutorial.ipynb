{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trade Smarter w/ Reinforcement Learning\n",
    "## A deep dive into TensorTrade - the Python framework for trading and investing using deep reinforcement learning\n",
    "\n",
    "Winning high stakes poker tournaments, beating world-class StarCraft players, and autonomously driving Tesla's futuristic sports cars. What do they all have in common? Each of these extremely complex tasks were long thought to be impossible by machines, until only recently being made possible through the massive advancements in deep reinforcement learning. Reinforcement learning is beginning to take over the world.\n",
    "\n",
    "A little over two months ago, I decided I wanted to be a part of the revolution, so I set out on a journey to create a profitable Bitcoin trading strategy using state-of-the-art deep reinforcement learning algorithms. While I made quite a bit of progress on that front, I realized that the tooling for this sort of project can be quite daunting to wrap your head around, and as such, it is very easy to get lost in the details.\n",
    "\n",
    "In between optimizing my previous library for distributed high-performance computing (HPC) systems; getting lost in endless pipelines of data and feature optimizations; and running my head in circles around efficient model set-up, tuning, training, and evaluation; I realized that there had to be a better way of doing things. After countless hours of researching existing projects, spending endless nights watching PyData conference talks, and having many back-and-forth conversations with the hundreds of members of the  RL trading Discord community, I realized there weren't any existing solutions that were all that good.\n",
    "\n",
    "There were many bits and pieces of great reinforcement learning trading systems spread across the inter-webs, but nothing solid and complete. For this reason, I've decided to create an open source Python framework for getting any trading strategy from idea to production, efficiently, using deep reinforcement learning. \n",
    "\n",
    "Enter TensorTrade. The idea was to create a highly modular framework for building efficient reinforcement learning trading strategies in a composable, maintainable way. Sounds like a mouthful of buzz-words if you ask me, so let's get into the meat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "TensorTrade is an open source Python framework for training, evaluating, and deploying robust trading strategies using deep reinforcement learning. The framework focuses on being highly composable and extensible, to allow the system to scale from simple trading strategies on a single CPU to complex investment strategies run on a distribution of HPC machines. Under the hood, the framework uses many of the APIs from existing machine learning frameworks to maintain high quality data pipelines and learning models.\n",
    "\n",
    "One of the main goals of TensorTrade is to enable fast experimentation with algorithmic trading strategies by leveraging the existing tools and pipelines provided by pandas, gym, sklearn, ray, keras, and tensorflow. It aims to simplify the process of testing and deploying robust trading agents using deep reinforcement learning, so you can focus on creating profitable strategies.\n",
    "\n",
    "## RL Primer\n",
    "\n",
    "In case your reinforcement learning chops are a bit rusty, let's quickly go over the basic concepts.\n",
    "\n",
    "Every reinforcement learning problem starts out with an environment and one or more agents that can interact with the environment.\n",
    "\n",
    "This technique is based off Markov Decision Processes (MDP) dating back to the 1950s.The agent will first observe the environment, then build a model of the current state and the expected value of actions within that environment. Based on that model, the agent will then take the action it has deemed as having the highest expected value.\n",
    "\n",
    "The agent will then be rewarded by an amount corresponding to the actual value of the action taken within the environment. The reinforcement learning agent can then, through the process of trial and error (i.e. reinforcement), improve its underlying model and take more rewarding actions over time.\n",
    "\n",
    "If you still need a bit of refreshment on the subject, there is a link to an article titled Introduction to Deep Reinforcement Learning in the references for this article, which is much more in-depth. Let's move on.\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "The following tutorial should provide enough examples to get you started with creating simple trading strategies using TensorTrade, although you will quickly see the framework is capable of handling much more complex configurations.\n",
    "\n",
    "## Installation\n",
    "\n",
    "TensorTrade requires Python 3.6 or later, so make sure you've got a valid version before pip installing the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.\u001b[0m\n",
      "Collecting tensortrade\n",
      "Requirement already satisfied: gin-config in /usr/local/lib/python2.7/site-packages (from tensortrade) (0.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python2.7/site-packages (from tensortrade) (1.16.4)\n",
      "Requirement already satisfied: gym in /usr/local/lib/python2.7/site-packages (from tensortrade) (0.14.0)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python2.7/site-packages (from tensortrade) (0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python2.7/site-packages (from tensortrade) (0.24.2)\n",
      "Requirement already satisfied: enum34; python_version < \"3\" in /usr/local/lib/python2.7/site-packages (from gin-config->tensortrade) (1.1.6)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/site-packages (from gin-config->tensortrade) (1.12.0)\n",
      "Requirement already satisfied: pyglet<=1.3.2,>=1.2.0 in /usr/local/lib/python2.7/site-packages (from gym->tensortrade) (1.3.2)\n",
      "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python2.7/site-packages (from gym->tensortrade) (1.2.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python2.7/site-packages (from gym->tensortrade) (1.2.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/site-packages (from sklearn->tensortrade) (0.20.4)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/site-packages (from pandas->tensortrade) (2019.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/site-packages (from pandas->tensortrade) (2.8.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python2.7/site-packages (from pyglet<=1.3.2,>=1.2.0->gym->tensortrade) (0.17.1)\n",
      "Installing collected packages: tensortrade\n",
      "Successfully installed tensortrade-0.0.1a1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensortrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorTrade Components\n",
    "\n",
    "At the core of TensorTrade are  trading strategies. Trading strategies combine reinforcement learning agents with composable trading logic in the form of a gym environment. A trading environment is made up of a set of modular components that can be mixed and matched to create highly diverse trading and investment strategies. I will explain this in further detail later, but for now it is enough to know the basics.\n",
    "\n",
    "The code snippets in this section should serve as guidelines for creating new components. There will likely be missing implementation details that will become more clear in a later section or as more components are defined.\n",
    "\n",
    "## Trading Strategy\n",
    "\n",
    "A TradingStrategy consists of a learning agent and one or more trading environments to tune, train, and evaluate on. If only one environment is provided, it will be used for tuning, training, and evaluating. Otherwise, a separate environment may be provided for each step.\n",
    "\n",
    "Trading environments are fully configurable gym environments with highly composable InstrumentExchange, FeaturePipeline, ActionStrategy, and RewardStrategy components. The InstrumentExchange provides observations to the environment and executes the agent's trades, the FeaturePipeline optionally transforms the exchange output into a more meaningful set of features before it is passed to the agent, the ActionStrategy converts the agent's actions into executable trades, and the RewardStrategy calculates the reward for each time step based on the agent's performance.\n",
    "\n",
    "If it seems a bit complicated now, trust me, it's not. That is all there is to it, now it's just a matter of composing each of these components into a complete strategy. Let's begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-04365641deec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorforceTradingStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m agent_spec = {\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"ppo_agent\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"step_optimizer\": {\n",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/strategies/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrading_strategy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTradingStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorforce_strategy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorforceStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/strategies/trading_strategy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrading_environment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTradingEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_pipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeaturePipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/environments/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrading_environment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTradingEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/environments/trading_environment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "from tensortrade.strategies import TensorforceTradingStrategy\n",
    "\n",
    "agent_spec = {\n",
    "    \"type\": \"ppo_agent\",\n",
    "    \"step_optimizer\": {\n",
    "        \"type\": \"adam\",\n",
    "        \"learning_rate\": 1e-4\n",
    "    },\n",
    "    \"discount\": 0.99,\n",
    "    \"likelihood_ratio_clipping\": 0.2,\n",
    "}\n",
    "network_spec = [\n",
    "    dict(type='dense', size=64, activation=\"tanh\"),\n",
    "    dict(type='dense', size=32, activation=\"tanh\")\n",
    "]\n",
    "\n",
    "strategy = TensorforceTradingStrategy(environment=environment,\n",
    "                                      agent_spec=agent_spec,\n",
    "                                      network_spec=network_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry if the agent_spec and network_spec seem a bit confusing now, I will go over them in more detail later on.\n",
    "\n",
    "## Trading Environment\n",
    "\n",
    "A trading environment is a reinforcement learning environment that follows OpenAI's gym.Env specification. This allows us to leverage many of the existing reinforcement learning models in our trading agent, if we'd like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2afb2d08ccb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTradingEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m environment = TradingEnvironment(exchange=exchange,\n\u001b[1;32m      3\u001b[0m                                  \u001b[0mfeature_pipeline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_pipeline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                  \u001b[0maction_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                  reward_strategy=reward_strategy)\n",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/environments/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrading_environment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTradingEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/environments/trading_environment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "from tensortrade.environments import TradingEnvironment\n",
    "environment = TradingEnvironment(exchange=exchange,\n",
    "                                 feature_pipeline=feature_pipeline,\n",
    "                                 action_strategy=action_strategy,\n",
    "                                 reward_strategy=reward_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the recommended use case is to plug a trading environment into a trading strategy, you can obviously use the trading environment separately, however you'd like.\n",
    "\n",
    "## Instrument Exchanges\n",
    "\n",
    "Instrument exchanges determine the universe of tradable instruments within a trading environment, return observations to the environment on each time step, and execute trades made within the environment. There are two types of instrument exchanges: live and simulated. \n",
    "\n",
    "Live exchanges are implementations of InstrumentExchange backed by live pricing data and a live trade execution engine. For example, CCXTExchange is a live exchange, which is capable of returning pricing data and executing trades on hundreds of live cryptocurrency exchanges, such as Binance and Coinbase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ccxt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-512829a0fddd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mccxt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexchanges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCCXTExchange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcoinbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mccxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoinbasepro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexchange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCCXTExchange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexchange\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoinbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_instrument\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ccxt'"
     ]
    }
   ],
   "source": [
    "import ccxt\n",
    "from tensortrade.exchanges.live import CCXTExchange\n",
    "\n",
    "coinbase = ccxt.coinbasepro()\n",
    "exchange = CCXTExchange(exchange=coinbase, base_instrument='USD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulated exchanges, on the other hand, are implementations of InstrumentExchange backed by simulated pricing data and trade execution. For example, FBMExchange is a simulated exchange, which generates pricing and volume data using fractional brownian motion (FBM). Since its price is simulated, the trades it executes must be simulated as well. The exchange uses a simple slippage model to simulate price and volume slippage on trades, though like almost everything in TensorTrade, this slippage model can easily be swapped out for something more complex.\n",
    "\n",
    "Though the FBMExchange generates fake price and volume data using a stochastic model, it is simply an implementation of SimulatedExchange. Under the hood, SimulatedExchange only requires a data_frame of price history to generate its simulations. This data_frame can either be provided by a coded implementation such as FBMExchange, or at runtime such as in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ab6312587256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexchanges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulated\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimulatedExchange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/btc_ohclv_1h.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexchange\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimulatedExchange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_instrument\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'USD'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/exchanges/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minstrument_exchange\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstrumentExchange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimulated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/exchanges/instrument_exchange.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mABCMeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrades\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrade\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensortrade.exchanges.simulated import SimulatedExchange\n",
    "\n",
    "df = pd.read_csv('./data/btc_ohclv_1h.csv')\n",
    "exchange = SimulatedExchange(data_frame=df, base_instrument='USD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Pipelines\n",
    "\n",
    "Feature pipelines are meant for transforming observations from the environment into meaningful features for an agent to learn from. If a pipeline has been set for a particular trading environment, then observations will be passed through the FeaturePipeline before being output to the agent. For example, a feature pipeline could normalize all price values, make a time series stationary, add a moving average column, and remove an unnecessary column, before the observation is returned to the agent.\n",
    "\n",
    "Feature pipelines can be initialized with an arbitrary number of comma-separated transformers. Each Transformer needs to be initialized with the set of columns to transform, or if nothing is passed, all columns will be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from tensortrade.features import FeaturePipeline\n",
    "from tensortrade.features.scalers import MinMaxNormalizer\n",
    "from tensortrade.features.stationarity import FractionalDifference\n",
    "\n",
    "normalize_price = MinMaxNormalizer([\"open\", \"high\", \"low\", \"close\"])\n",
    "difference_all = FractionalDifference(difference_order=0.6)\n",
    "feature_pipeline = FeaturePipeline(normalize_price, difference_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature pipeline normalizes the price values between 0 and 1, before making the entire time series stationary by fractionally differencing consecutive values.\n",
    "\n",
    "## Action Strategies\n",
    "\n",
    "Action strategies define the action space of the environment and convert an agent's actions into executable trades. For example, if we were using a discrete action space of 3 actions (0 = hold, 1 = buy, 2 = sell), our learning agent does not need to know that returning an action of 1 is equivalent to buying an instrument. Rather, our agent needs to know the reward for returning an action of 1 in specific circumstances, and can leave the implementation details of converting actions to trades to the ActionStrategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-af6dea292cad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiscreteActionStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m action_strategy = DiscreteActionStrategy(n_actions=20, \n\u001b[1;32m      4\u001b[0m                                          instrument_symbol='BTC')\n",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/actions/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maction_strategy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActionStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDTypeString\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTradeActionUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcontinuous_action_strategy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContinuousActionStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdiscrete_action_strategy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiscreteActionStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/actions/action_strategy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mABCMeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrades\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrade\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "from tensortrade.actions import DiscreteActionStrategy\n",
    "\n",
    "action_strategy = DiscreteActionStrategy(n_actions=20, \n",
    "                                         instrument_symbol='BTC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This discrete action strategy uses 20 discrete actions, which equates to 4 discrete amounts for each of the 5 trade types (market buy/sell, limit buy/sell, and hold). E.g. [0,5,10,15]=hold, 1=market buy 25%, 2=market sell 25%, 3=limit buy 25%, 4=limit sell 25%, 6=market buy 50%, 7=market sell 50%, etc…\n",
    "\n",
    "## Reward Strategies\n",
    "\n",
    "Reward strategies receive the trade taken at each time step and return a float, corresponding to the benefit of that specific action. For example, if the action taken this step was a sell that resulted in positive profits, our RewardStrategy could return a positive number to encourage more trades like this. On the other hand, if the action was a sell that resulted in a loss, the strategy could return a negative reward to teach the agent not to make similar actions in the future. A version of this example algorithm is implemented in SimpleProfitStrategy, however more complex strategies can obviously be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.rewards import SimpleProfitStrategy\n",
    "\n",
    "reward_strategy = SimpleProfitStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple profit strategy returns a reward of -1 for not holding a trade, 1 for holding a trade, 2 for purchasing an instrument, and a value corresponding to the (positive/negative) profit earned by a trade if an instrument was sold.\n",
    "\n",
    "## Learning Agents\n",
    "\n",
    "Up until this point, we haven't seen the \"deep\" part of the deep reinforcement learning framework. This is where learning agents come in. Learning agents are where the math (read: magic) happens.\n",
    "\n",
    "At each time step, the agent takes the observation from the environment as input, runs it through its underlying model (a neural network most of the time), and outputs the action to take. For example, the observation might be the previous open, high, low, and close price from the exchange. The learning model would take these values as input and output a value corresponding to the action to take, such as buy, sell, or hold.\n",
    "\n",
    "It is important to remember the learning model has no intuition of the prices or trades being represented by these values. Rather, the model is simply learning which values to output for specific input values or sequences of input values, to earn the highest reward.\n",
    "\n",
    "In this example, we will be using the Tensorforce library to provide learning agents to our trading strategy, although the TensorTrade framework is compatible with many reinforcement learning libraries such as Ray's RLLib, OpenAI's Baselines (or the much better maintained Stable Baselines), Intel's Coach, or anything from the TensorFlow line such as TF Agents.\n",
    "\n",
    "It is possible that custom learning agents will be added to this framework in the future, though it will always be a goal of the framework to be interoperable with as many existing reinforcement learning libraries as possible, since there is so much concurrent growth in the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorforce'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ce4735ae6a7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorforce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m agent = Agent.from_spec(spec=agent_spec,\n\u001b[1;32m      4\u001b[0m                         kwargs=dict(states=environment.states,            \n\u001b[1;32m      5\u001b[0m                                     \u001b[0mactions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorforce'"
     ]
    }
   ],
   "source": [
    "from tensorforce.agents import Agent\n",
    "\n",
    "agent = Agent.from_spec(spec=agent_spec,\n",
    "                        kwargs=dict(states=environment.states,            \n",
    "                                    actions=environment.actions,             \n",
    "                                    network=network_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This example uses the tensorforce library to provide learning agents. This is not required to use TensorTrade, though it is required for this tutorial.\n",
    "\n",
    "# Putting it All Together\n",
    "\n",
    "Now that we know about each component that makes up a TradingStrategy, let's build and evaluate one.\n",
    "\n",
    "For a quick recap, a TradingStrategy is made up of a TradingEnvironment and a learning agent. A TradingEnvironment is a gym environment that takes an InstrumentExchange, an ActionStrategy, a RewardStrategy, and an optional FeaturePipeline, and returns observations and rewards that the learning agent can be trained and evaluated on.\n",
    "\n",
    "## Creating an Environment\n",
    "\n",
    "The first step is to create a TradingEnvironment using the components outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-389917054bd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexchanges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulated\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFBMExchange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxNormalizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstationarity\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFractionalDifference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeaturePipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimpleProfitStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/exchanges/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minstrument_exchange\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstrumentExchange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimulated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/exchanges/instrument_exchange.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mABCMeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrades\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrade\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "from tensortrade.exchanges.simulated import FBMExchange\n",
    "from tensortrade.features.scalers import MinMaxNormalizer\n",
    "from tensortrade.features.stationarity import FractionalDifference\n",
    "from tensortrade.features import FeaturePipeline\n",
    "from tensortrade.rewards import SimpleProfitStrategy\n",
    "from tensortrade.actions import DiscreteActionStrategy\n",
    "from tensortrade.environments import TradingEnvironment\n",
    "\n",
    "exchange = FBMExchange(base_instrument='BTC', timeframe='1h')\n",
    "normalize_price = MinMaxNormalizer([\"open\", \"high\", \"low\", \"close\"])\n",
    "difference = FractionalDifference(difference_order=0.6)\n",
    "feature_pipeline = FeaturePipeline(normalize_price, difference)\n",
    "reward_strategy = SimpleProfitStrategy()\n",
    "action_strategy = DiscreteActionStrategy(n_actions=20, instrument_symbol='ETH/BTC')\n",
    "\n",
    "environment = TradingEnvironment(exchange=exchange,\n",
    "                                 feature_pipeline=feature_pipeline,\n",
    "                                 action_strategy=action_strategy,\n",
    "                                 reward_strategy=reward_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple enough, now environment is a gym environment that can be used by any compatible trading strategy or learning agent.\n",
    "\n",
    "## Defining the Agent\n",
    "\n",
    "Now that the environment is set up, it's time to create our learning agent. Again, we will be using Tensorforce for this, but feel free to drop in any other reinforcement learning agent here.\n",
    "\n",
    "Since we are using TensorforceTradingStrategy, all we need to do is provide an agent specification and a network specification for the underlying neural network to be trained. For this example, we will be using a simple proximal policy optimization (PPO) agent and a simple dense network.\n",
    "\n",
    "For more examples of agent and network specifications, see the Tensorforce Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_spec = {\n",
    "    \"type\": \"ppo_agent\",\n",
    "    \"step_optimizer\": {\n",
    "        \"type\": \"adam\",\n",
    "        \"learning_rate\": 1e-4\n",
    "    },\n",
    "    \"discount\": 0.99,\n",
    "    \"likelihood_ratio_clipping\": 0.2,\n",
    "}\n",
    "\n",
    "network_spec = [\n",
    "    dict(type='dense', size=64, activation=\"tanh\"),\n",
    "    dict(type='dense', size=32, activation=\"tanh\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Strategy\n",
    "\n",
    "Creating our trading strategy is as simple as plugging in the environment, the agent specification, and the network specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-578d30e37888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategies\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorforceTradingStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m strategy = TensorforceTradingStrategy(environment=environment,\n\u001b[1;32m      4\u001b[0m                                       \u001b[0magent_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                       network_spec=network_spec)\n",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/strategies/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrading_strategy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTradingStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorforce_strategy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorforceStrategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/strategies/trading_strategy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrading_environment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTradingEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_pipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeaturePipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/environments/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrading_environment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTradingEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Hedger/tensortrade/tensortrade/environments/trading_environment.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "from tensortrade.strategies import TensorforceTradingStrategy\n",
    "\n",
    "strategy = TensorforceTradingStrategy(environment=environment,\n",
    "                                      agent_spec=agent_spec,\n",
    "                                      network_spec=network_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to train the strategy (i.e. train the agent on the current environment), all we need to do is pass should_train=True to strategy.run()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strategy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-5947a64badd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperformance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshould_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'strategy' is not defined"
     ]
    }
   ],
   "source": [
    "performance = strategy.run(steps=100000, should_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voila! Three hours later you will see the results of how your agent has done! If this feedback loop is a bit slow for you, you can pass a callback function to run function, which will be called at the end of each episode. The function will pass in a data frame containing the agent's progress that episode, and expects a bool in return. If True, the agent will continue training, otherwise, the agent will stop and return its performance.\n",
    "\n",
    "TODO: Example Performance\n",
    "\n",
    "## Saving and Restoring\n",
    "\n",
    "All trading strategies are capable of saving their agent to a file, for later restoring. The environment is not saved, as it does not have state that we care about preserving. To save our TensorflowTradingStrategy to a file, we just to need to provide the path of file to our strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'strategy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-02a0c4d32486>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../agents/ppo_btc_1h\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'strategy' is not defined"
     ]
    }
   ],
   "source": [
    "strategy.save_agent(path=\"../agents/ppo_btc_1h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specific strategy saves multiple files, including a directory of models to the path provided.\n",
    "\n",
    "To restore the agent from the file, we first need to instantiate our strategy, before calling restore_agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TensorforceTradingStrategy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-225103250f7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorforceTradingStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../agents/ppo_btc/1h\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TensorforceTradingStrategy' is not defined"
     ]
    }
   ],
   "source": [
    "strategy = TensorforceTradingStrategy(environment=environment)\n",
    "strategy.restore_agent(path=\"../agents/ppo_btc/1h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our strategy is now restored back to its previous state, and ready to be used again. Let's see how it does.\n",
    "\n",
    "## Strategy Evaluation\n",
    "\n",
    "To evaluate our strategy's performance on unseen data, we will need to run it on a new environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pd' from 'pandas' (//anaconda3/lib/python3.7/site-packages/pandas/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-152ec99bc5e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTradingEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexchanges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulated\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSimulatedExchange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./btc_ohlcv_1h.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'pd' from 'pandas' (//anaconda3/lib/python3.7/site-packages/pandas/__init__.py)"
     ]
    }
   ],
   "source": [
    "from pandas import pd\n",
    "from tensortrade.environments import TradingEnvironment\n",
    "from tensortrade.exchanges.simulated import SimulatedExchange\n",
    "\n",
    "df = pd.read_csv('./btc_ohlcv_1h.csv')\n",
    "exchange = SimulatedExchange(data_frame=df, base_instrument='BTC')\n",
    "environment = TradingEnvironment(exchange=exchange,\n",
    "                                 feature_pipeline=feature_pipeline,\n",
    "                                 action_strategy=action_strategy,\n",
    "                                 reward_strategy=reward_strategy)\n",
    "\n",
    "strategy.environment = environment\n",
    "\n",
    "test_performance = strategy.run(episodes=1, should_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning a Strategy\n",
    "\n",
    "TODO:\n",
    "\n",
    "## Live Trading\n",
    "\n",
    "TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ccxt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-89988abfbe52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mccxt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironments\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTradingEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensortrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexchanges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCCXTExchange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcoinbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mccxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoinbasepro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ccxt'"
     ]
    }
   ],
   "source": [
    "import ccxt\n",
    "from tensortrade.environments import TradingEnvironment\n",
    "from tensortrade.exchanges.live import CCXTExchange\n",
    "\n",
    "coinbase = ccxt.coinbasepro(...)\n",
    "exchange = CCXTExchange(exchange=coinbase,\n",
    "                        base_instrument='USD', \n",
    "                        timeframe='1h')\n",
    "\n",
    "environment = TradingEnvironment(exchange=exchange,\n",
    "                                 feature_pipeline=feature_pipeline,\n",
    "                                 action_strategy=action_strategy,\n",
    "                                 reward_strategy=reward_strategy)\n",
    "\n",
    "strategy = TradingStrategy.restore('Trained_PPO_agent.json')\n",
    "strategy.environment = environment\n",
    "\n",
    "test_perf = strategy.evaluate(steps=0, callback=trading_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts\n",
    "\n",
    "TensorTrade is a powerful framework capable of building highly modular, high performance trading systems. It is fairly simple and easy to experiment with new trading and investment strategies, while allowing you to leverage components from one strategy in another. But don't take my word for it, create a strategy of your own and start teaching your robots to take over the world!\n",
    "\n",
    "While this tutorial should be enough to get you started, there is still quite a lot more to learn if you want to create a profitable trading strategy. I encourage you to head over to the Github and dive into the codebase, or take a look at our documentation at tensortrade.org. There is also quite an active Discord community with over 750 total members, so if you have questions, feedback, or feature requests, feel free to drop them there!\n",
    "\n",
    "I've gotten the project to a highly usable state. Though, my time is limited, and I believe there are many of you out there who could make valuable contributions to the open source codebase. So if you are a developer or data scientist with an interest in building state-of-the-art trading systems, I'd love to see you open a pull request, even if its just a simple test case!\n",
    "\n",
    "Others have asked how they can contribute to the project without writing code. There are currently two ways that you can do that. The first is to help write documentation for the existing code, which you can get paid to do. If you'd like to do this, please talk to me on Discord. The other way to contribute is to sponsor this project either on Patreon or with BTC/ETH donations. Your support means a lot to me and allows me to spend more of my free time working on developing the framework and writing articles like this.\n",
    "\n",
    "Thanks for reading! As always, all of the code for this tutorial can be found on my GitHub. Leave a comment below if you have any questions or feedback, I'd love to hear from you! I can also be reached on Twitter at @notadamking.\n",
    "\n",
    "## References\n",
    "\n",
    "###Introduction to Deep Reinforcement Learning\n",
    "\n",
    "https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199\n",
    "\n",
    "### Policy Gradient Algorithms\n",
    "\n",
    "https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#reinforce"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
