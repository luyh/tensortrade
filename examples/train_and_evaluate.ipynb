{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example \"dumb\" trading agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = warn\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "from tensortrade.environments import TradingEnvironment\n",
    "from tensortrade.exchanges.simulated import FBMExchange\n",
    "from tensortrade.actions import DiscreteActionStrategy\n",
    "from tensortrade.rewards import SimpleProfitStrategy\n",
    "\n",
    "exchange = FBMExchange()\n",
    "action_strategy = DiscreteActionStrategy()\n",
    "reward_strategy = SimpleProfitStrategy()\n",
    "\n",
    "env = TradingEnvironment(exchange=exchange,\n",
    "                         action_strategy=action_strategy,\n",
    "                         reward_strategy=reward_strategy)\n",
    "\n",
    "obs = env.reset()\n",
    "sell_price = 1e9\n",
    "stop_price = -1\n",
    "\n",
    "print('Initial portfolio: ', exchange.portfolio)\n",
    "\n",
    "for i in range(1000):\n",
    "    action = 0 if obs['close'] < sell_price else 18\n",
    "    action = 19 if obs['close'] < stop_price else action\n",
    "    \n",
    "    if i == 0 or portfolio['BTC'] == 0:\n",
    "        action = 16\n",
    "        sell_price = obs['close'] + (obs['close'] / 50)\n",
    "        stop_price = obs['close'] - (obs['close'] / 50)\n",
    "    \n",
    "    obs, reward, done, info = env.step(action)\n",
    "    executed_trade = info['executed_trade']\n",
    "    filled_trade = info['filled_trade']\n",
    "    portfolio = exchange.portfolio\n",
    "    \n",
    "    print('Obs: ', obs)\n",
    "    print('Reward: ', reward)\n",
    "    print('Portfolio: ', portfolio)\n",
    "    print('Trade executed: ', executed_trade.trade_type, executed_trade.price, executed_trade.amount)\n",
    "    print('Trade filled: ', filled_trade.trade_type, filled_trade.price, filled_trade.amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0827 22:51:22.413044 4475143616 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/tensorforce/core/optimizers/tf_optimizer.py:31: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
      "\n",
      "W0827 22:51:22.413619 4475143616 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/tensorforce/core/optimizers/tf_optimizer.py:32: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n",
      "W0827 22:51:22.414273 4475143616 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/tensorforce/core/optimizers/tf_optimizer.py:33: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0827 22:51:22.980015 4475143616 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0827 22:51:22.980869 4475143616 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/tensorforce/core/optimizers/tf_optimizer.py:35: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "W0827 22:51:22.981399 4475143616 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/tensorforce/core/optimizers/tf_optimizer.py:36: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
      "\n",
      "W0827 22:51:22.985899 4475143616 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/tensorforce/core/networks/layer.py:231: The name tf.layers.AveragePooling1D is deprecated. Please use tf.compat.v1.layers.AveragePooling1D instead.\n",
      "\n",
      "W0827 22:51:22.986762 4475143616 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/tensorforce/core/networks/layer.py:232: The name tf.layers.AveragePooling2D is deprecated. Please use tf.compat.v1.layers.AveragePooling2D instead.\n",
      "\n",
      "W0827 22:51:22.987447 4475143616 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/tensorforce/core/networks/layer.py:233: The name tf.layers.AveragePooling3D is deprecated. Please use tf.compat.v1.layers.AveragePooling3D instead.\n",
      "\n",
      "W0827 22:51:22.987987 4475143616 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/tensorforce/core/networks/layer.py:234: The name tf.layers.BatchNormalization is deprecated. Please use tf.compat.v1.layers.BatchNormalization instead.\n",
      "\n",
      "W0827 22:51:22.988898 4475143616 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/tensorforce/core/networks/layer.py:235: The name tf.layers.Conv1D is deprecated. Please use tf.compat.v1.layers.Conv1D instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 1 after 1665 timesteps (reward: -2308.5202051242336)\n",
      "Finished episode 2 after 1665 timesteps (reward: -1283.0615648964729)\n",
      "Finished episode 3 after 1665 timesteps (reward: -1776.6452968587246)\n",
      "Finished episode 4 after 1665 timesteps (reward: -1641.6904180166866)\n",
      "Finished episode 5 after 1665 timesteps (reward: -1462.7058570205527)\n",
      "Finished episode 6 after 1665 timesteps (reward: -1518.935115373318)\n",
      "Finished episode 7 after 1665 timesteps (reward: -1226.6336069671968)\n",
      "Finished episode 8 after 1665 timesteps (reward: -1403.566511120577)\n",
      "Finished episode 9 after 1665 timesteps (reward: -1706.5460019310076)\n",
      "Finished episode 10 after 1665 timesteps (reward: -1563.9667490620047)\n",
      "Finished episode 11 after 1665 timesteps (reward: -1353.4441234147248)\n",
      "Finished episode 12 after 1665 timesteps (reward: -1558.8286621450547)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = warn\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from tensorforce.agents import Agent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "from tensortrade.environments import TradingEnvironment\n",
    "from tensortrade.exchanges.simulated import FBMExchange\n",
    "from tensortrade.actions import DiscreteActionStrategy\n",
    "from tensortrade.rewards import SimpleProfitStrategy\n",
    "\n",
    "exchange = FBMExchange(times_to_generate=100000)\n",
    "action_strategy = DiscreteActionStrategy()\n",
    "reward_strategy = SimpleProfitStrategy()\n",
    "\n",
    "env = TradingEnvironment(exchange=exchange,\n",
    "                         action_strategy=action_strategy,\n",
    "                         reward_strategy=reward_strategy,\n",
    "                         feature_pipeline=None)\n",
    "\n",
    "agent_config = {\n",
    "    \"type\": \"dqn_agent\",\n",
    "\n",
    "    \"update_mode\": {\n",
    "        \"unit\": \"timesteps\",\n",
    "        \"batch_size\": 64,\n",
    "        \"frequency\": 4\n",
    "    },\n",
    "    \n",
    "    \"memory\": {\n",
    "        \"type\": \"replay\",\n",
    "        \"capacity\": 10000,\n",
    "        \"include_next_states\": True\n",
    "    },\n",
    "\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"clipped_step\",\n",
    "        \"clipping_value\": 0.1,\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"adam\",\n",
    "            \"learning_rate\": 1e-3\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"discount\": 0.999,\n",
    "    \"entropy_regularization\": None,\n",
    "    \"double_q_model\": True,\n",
    "\n",
    "    \"target_sync_frequency\": 1000,\n",
    "    \"target_update_weight\": 1.0,\n",
    "\n",
    "    \"actions_exploration\": {\n",
    "        \"type\": \"epsilon_anneal\",\n",
    "        \"initial_epsilon\": 0.5,\n",
    "        \"final_epsilon\": 0.,\n",
    "        \"timesteps\": 1000000000\n",
    "    },\n",
    "\n",
    "    \"saver\": {\n",
    "        \"directory\": None,\n",
    "        \"seconds\": 600\n",
    "    },\n",
    "    \"summarizer\": {\n",
    "        \"directory\": None,\n",
    "        \"labels\": [\"graph\", \"total-loss\"]\n",
    "    },\n",
    "    \"execution\": {\n",
    "        \"type\": \"single\",\n",
    "        \"session_config\": None,\n",
    "        \"distributed_spec\": None\n",
    "    }\n",
    "}\n",
    "\n",
    "network_spec = [\n",
    "    dict(type='dense', size=64),\n",
    "    dict(type='dense', size=32)\n",
    "]\n",
    "\n",
    "agent = Agent.from_spec(\n",
    "        spec=agent_config,\n",
    "        kwargs=dict(\n",
    "            states=env.states,\n",
    "            actions=env.actions,\n",
    "            network=network_spec,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create the runner\n",
    "runner = Runner(agent=agent, environment=env)\n",
    "\n",
    "\n",
    "# Callback function printing episode statistics\n",
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "                                                                                 reward=r.episode_rewards[-1]))\n",
    "    return True\n",
    "\n",
    "\n",
    "# Start learning\n",
    "runner.run(episodes=300, max_episode_timesteps=10000, episode_finished=episode_finished)\n",
    "runner.close()\n",
    "\n",
    "# Print statistics\n",
    "print(\"Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.\".format(\n",
    "    ep=runner.episode,\n",
    "    ar=np.mean(runner.episode_rewards))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
